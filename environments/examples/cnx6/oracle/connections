# Inventory file tested for Connections 6.5 and 6.5.0.1 (should work with older as well)
# By default, WAS FP 18 gets installed with Connections 7. This file shows how to overwrite 
# WAS FP to 16 and how to install Connections 6.5 instead.

[dmgr]
connections1.internal.example.com

[was_servers]
connections1.internal.example.com

[nfs_servers]
connections1.internal.example.com

[ihs_servers]
connections1.internal.example.com

[oracle_servers]
db1.internal.example.com

[ldap_servers]
db1.internal.example.com

[ldap_servers:vars]
# Set to false if you want to skip adding fake LDAP users (for example, if you are setting up production environment and you don't need them)
setup_fake_ldap_users=True
# Number of fake users that will be created.
ldap_nr_of_users=2500
# ID of user(s) if creating dummy users for the demo purposes. First one becomes Connections admin, and gets ldap_user_admin_password. All others get
# ldap_user_password as password. Users will be enumerated from ldap_userid1 to ldap_userid2500. Enough for simple performance testing.
ldap_userid="fakeuser"
ldap_user_password="password"
ldap_user_admin_password="password"

[ihs_servers:vars]
iim_bin_file="agent.installer.linux.gtk.x86_64_1.9.1003.20200730_2125.zip"
iim_version="1.9.1003.20200730_2125"
ihs_username=ihsadmin
ihs_password=password

# This will install JDBC drivers for Oracle on all WAS servers. If this section is missing, JDBC drivers won't be installed.
[was_servers:vars]
setup_oracle_jdbc=True

[all:vars]
oracle_download_location=http://installer1.internal.example.com:8001/Oracle
tdi_download_location=http://installer1.internal.example.com:8001/TDI
iim_repository_url=http://installer1.internal.example.com:8001/was855
was_repository_url=http://installer1.internal.example.com:8001/was855
ihs_repository_url=http://installer1.internal.example.com:8001/was855
cnx_docs_download_location=http://installer1.internal.example.com:8001/Docs

# Connections 6.5 and Wizards for 6.5 live in Connections6.5 subfolder on 
# installer1.internal.example.com:/data/packages/Connections6.5 
cnx_repository_url=http://installer1.internal.example.com:8001/Connections6.5
connections_wizards_download_location=http://installer1.internal.example.com:8001/Connections6.5

# WAS FP 16 fixes live in installer1.internal.example.com:/data/packages/was855FP16
was_fixes_repository_url=http://installer1.internal.example.com:8001/was855FP16
ihs_fixes_repository_url=http://installer1.internal.example.com:8001/was855FP16

# WAS FP 18 live in installer1.internal.example.com:/data/packages/was855FP18
#was_fixes_repository_url=http://installer1.internal.example.com:8001/was855FP18
#ihs_fixes_repository_url=http://installer1.internal.example.com:8001/was855FP18

# The next two variables are fixing IBM Java version to one distributed with FP 16
java_version="8.0.5017.20180726_2118"
java_files={ 'file_name': '8.0.5.17-WS-IBMWASJAVA-Linux.zip' }

# The variables below are referencing the file names for WAS ND and IHS as named in Flexnet. Uncomment it if you are using Flexnet packages.

# Change to Filenames used in Flexnet
#was_files[]={"file_name": "CIK2HML.zip", "check_sum": "b1333962ba4b25c8632c7e4c82b472350337e99febac8f70ffbd551ca3905e83" }
#was_files[]={"file_name": "CIK2JML.zip", "check_sum": "b73ae070656bed6399a113c2db9fb0abaf5505b0d41c564bf2a58ce0b1e0dcd2" }
#was_files[]={"file_name": "CIK2IML.zip", "check_sum": "440b7ed82089d43b1d45c1e908bf0a1951fed98f2542b6d37c8b5e7274c6b1c9" }
#
#ihs_files[]={"file_name": "CIK1VML.zip", "check_sum": "d63c59de4a5548e3d26e71fefb76193d41ac7585bc450c1e504287e0a6f746c9"}
#ihs_files[]={"file_name": "CIK1WML.zip", "check_sum": "ac00e7ab43cc528fe7f3ccd69aeb6564a2e738e7bc6e30e71fd2e0d4bd64f39e"}
#ihs_files[]={"file_name": "CIK1XML.zip", "check_sum": "94e3d9b70b139ad5fa0578da6857b295c5d2370c1b6ecb544c1e5757406fec90"}

# The variables below are specifying the specific version of FP16 and files needed
# to install it. Those files live in the repositories mentioned above.
was_fp_version="8.5.5016.20190801_0951"
was_fp_files[]={"file_name":"8.5.5-WS-WASSupplements-FP016-part1.zip"}
was_fp_files[]={"file_name":"8.5.5-WS-WASSupplements-FP016-part2.zip"}
was_fp_files[]={"file_name":"8.5.5-WS-WASSupplements-FP016-part3.zip"}

ihs_version="8.5.5016.20190801_0951"
ihs_fp_files[]={"file_name":"8.5.5-WS-WASSupplements-FP016-part1.zip"}
ihs_fp_files[]={"file_name":"8.5.5-WS-WASSupplements-FP016-part2.zip"}
ihs_fp_files[]={"file_name":"8.5.5-WS-WASSupplements-FP016-part3.zip"}

# Uncomment this if you don't want to attempt NFS mounts for your HCL Connections installation. 
# By default, this is set to false.
#skip_nfs_mount_for_connections=true

# This variable is used exclusively to decide which response file to render for pre version 7
cnx_major_version="6"

# Any package name can be specified here. This is how we call latest build for 6.5.0.1
cnx_package="HCL_Connections_6.5_lin.tar"

# This variable is used by Wizards to decide if IC360 DB is getting installed or not. For pre 7 it is not installed.
db_version="6.5"

# This variable specifies the package name of Connections Wizards being used to spin up the database.
connections_wizards_package_name="HCL_Connections_6.5_wizards_lin_aix.tar"

# This variable will let automation figure out specific product ID of the Connections that you are installing. Otherwise,
# if disabled, automation is going to expect explicitely specified version that is being installed. This enables basically running Connections updates/upgrades as well.
cnx_updates_enabled=True

was_username=wasadmin
was_password=password
#
ldap_server=db1.internal.example.com
ldap_alias=db1
ldap_repo=LDAP_PRODUCTION1
ldap_bind_user=cn=Admin,dc=cnx,dc=pnp-hcl,dc=com
ldap_bind_pass=password
ldap_realm=dc=cnx,dc=pnp-hcl,dc=com
ldap_login_properties=uid;mail
#
dmgr_hostname=connections1.internal.example.com
was_domainname=.internal.example.com
domain_name=.internal.example.com
cnx_domainname=.internal.example.com
smtp_hostname=localhost
#
ihs_password=password

# This needs to be a name of at least one IHS server already added as a node to DMGR. 
cnx_webserver="connections1"

# This is the URL of load balancer pointing towards your Kubernetes cluster in case you are using or going to use Component Pack. Will be used inside httpd.conf
cnx_component_pack_ingress="web1.internal.example.com"

# This is the URL of your Connections installation. Will be used for setting dynamicHost. 
cnx_application_ingress="web1.example.com"

# This is the mandatory variable. This user doesn't have to exist, but is mandatory during HCL Connections response
# file generation. connections_admin will become, as the name suggests, administrator of HCL Connections. 
connections_admin=fakeuser1

# This is also mandatory variable for HCL Connections installation. If you are creating fake users, it will be also
# used as their mail sufix. Beside that, it is used by ICXT component configuration to get a HCL Connections administrator
# data during ICXT configuration. 
ldap_user_mail_domain="connections.example.com"

cnx_node_list=['connections1']
cnx_hostname_list=['connections1.internal.example.com']
ihs_hostname_list=['connections1.internal.example.com']

common_app={ 'cluster_name': 'App', 'nodes':['connections1']}
communities_app={ 'cluster_name': 'App', 'nodes':['connections1']}
news_app={ 'cluster_name': 'App', 'nodes':['connections1']}
widgets_app={ 'cluster_name': 'App', 'nodes':['connections1']}

homepage_app={ 'cluster_name': 'App', 'nodes':['connections1']}
moderation_app={ 'cluster_name': 'App', 'nodes':['connections1']}
rte_app={ 'cluster_name': 'App', 'nodes':['connections1']}

activities_app={ 'cluster_name': 'App', 'nodes':['connections1']}
blogs_app={ 'cluster_name': 'App', 'nodes':['connections1']}
dogear_app={ 'cluster_name': 'App', 'nodes':['connections1']}
files_app={ 'cluster_name': 'App', 'nodes':['connections1']}
forums_app={ 'cluster_name': 'App', 'nodes':['connections1']}
metrics_app={ 'cluster_name': 'App', 'nodes':['connections1']}
mobile_app={ 'cluster_name': 'App', 'nodes':['connections1']}
wikis_app={ 'cluster_name': 'App', 'nodes':['connections1']}

push_app={ 'cluster_name': 'App', 'nodes':['connections1']}

profiles_app={ 'cluster_name': 'App', 'nodes':['connections1']}

search_app={ 'cluster_name': 'App', 'nodes':['connections1']}

# The list of clusters that will start automatically. 
connections_auto_start_clusters='App'

# The list of clusters for which you want to enable Prometheus JMX exporter. Choose different port for each cluster. Be careful not to clash with WAS ports. 
connections_jmx_clusters=[{"name":"App","port":"8080"}]

# If enabled, Prometheus JMX reported will be installed and set for connections_jmx_clusters
# enable_prometheus_jmx_exporter=True
db_username=EMPINST
db_password=password
db_hostname=db1.internal.example.com
db_port=1521
db_jdbc_file=/opt/oracle/product/19.0.0/db_1/jdbc/lib
db_type=Oracle

# This variable, if uncommented, is used exclusively by Connections Wizards to recreate the databases; if uncommented and
# Connections Wizards is invoked by this automation on the system which already has the databases properly set, automation
# is going to invoke dropDb.sql scripts for the databases already created and recreate the schemas. Please use with caution.
#cnx_force_repopulation=True

activities_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'OAUSER' }
blogs_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'BLOGSUSER' }
dogear_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'DOGEARUSER' }
communities_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'SNCOMMUSER' }
files_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'FILESUSER' }
forums_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'DFUSER' }
homepage_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'HOMEPAGEUSER' }
metrics_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'METRICSUSER' }
mobile_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'MOBILEUSER' }
profiles_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'PROFUSER' }
push_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'PNSUSER' }
wikis_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'WIKISUSER' }
icec_db={ 'name': 'LSCONN', 'server': 'db1.internal.example.com', 'user': 'ICEC' }

cnx_enable_moderation=true
cnx_enable_invite=true

# The next three variables are intended only for development purposes and there is no guarantee what can be unintentionally broken if using automation to remove stuff in a very brute force way. Keep them false if possible.

# This variable, if enabled, is letting playbooks/hcl/cleanup/cleanup-db2.yml remove DB2 brute force from the system. If the variable is set to false, running those cleanup scripts would just skip doing anything. This is intended for development purposes only!
#force_destroy_db2=True

# This variable, if enabled, is letting playbooks/hcl/cleanup/cleanup-ihs.yml remove IHS processes and related folders only. If set to false, running cleanup playbook will not do anything. This is intended for development purposes only!
#force_destroy_ihs=True

# This variable, if enabled, is letting playbooks/hcl/cleanup/cleanup-was.yml remove WAS & Connections in a very brute force way. If variable is set to false, running playbook itself will not affect anything. This is intended for development purposes only!
#force_destroy_websphere=True
