# Kubernetes masters. First in the group will become primary Kubernetes master, and used later through the scripts as such.
[k8s_masters]
cp1.internal.example.com

# List of all Kubernetes workers. Those workers will be joined to cluster of masters. 
[k8s_workers]
cp1worker1.internal.example.com
cp1worker2.internal.example.com

# On this host (those hosts) Helm will be set. By default 3.4.1
[k8s_admin]
cp1.internal.example.com

# If you are setting up Docker Registry, first node on the list will get the registry set, certificates created, and all the rest will trust this certificate
# and be able to docker login to the Registry itself.
[docker_registry]
cp1.internal.example.com
cp1worker1.internal.example.com
cp1worker2.internal.example.com

# If used, this is where your Haproxy lives.
[k8s_load_balancers]
web1.internal.example.com

# Here will all Component Pack installation steps happen. Can be same as K8S master if masters are available, otherwise any host with kubectl and helm
[component_pack_master]
cp1.internal.example.com

# If you are setting NFS as well for your PV(C)s, first server on the list is going to be NFS master. Add all Kubernetes nodes to the list. 
[nfs_servers]
cp1.internal.example.com
cp1worker1.internal.example.com
cp1worker2.internal.example.com

# DMGR and WAS nodes are needed for some of installation and post installation steps which include Connections configuration changes.
[dmgr]
connections1.internal.example.com

[was_servers]
connections1.internal.example.com

# This will ensure that Haproxy doesn't clash with Nginx ports when collocated.
[k8s_load_balancers:vars]
main_port='81'
main_ssl_port='444'

# Ensures if you are setting up DR or not. In any case, it will use docker_registry_url as the URL to your Docker Registry.
[docker_registry:vars]
setup_docker_registry=true

[component_pack_master:vars]
# If set to true, number of pods in connections namespace will be scaled to the number of total workers in k8s_workers
enable_pod_auto_scaling=True
# Location and name of Component Pack package
component_pack_download_location="installer1.internal.example.com:8001/cp"
component_pack_package_name="hybridcloud_6501.zip"
# If you are using different domains, this is needed to set up ingresses properly.
ce_on_prem=False
ingress_multi_domain_enabled=true
# If set to true, Ansible will not check if expected pods are in Running state
skip_pod_checks=true

# Variables bellow are enabling only the steps needed (and existing) in Component Pack for 6.5.0.1.
#setup_installation=false
#setup_images=false
#setup_credentials=false
#setup_connections_volumes=false
#setup_psp=false
#setup_bootstrap=false
#setup_connections_env=false
#setup_infrastructure=false
#setup_customizer=false
setup_elasticsearch=True
setup_elasticsearch7=false
setup_teams=false
setup_tailored_exp=false
#setup_ingress=false
#setup_ingress_rule=false
#setup_community_ingress=false
#setup_orientme=false
#setup_dashboards=false
#setup_elasticstack=false
#setup_sanity=false
#setup_kudosboards=false
#setup_prometheus_operator=false
setup_outlook_addin=false
#enable_es_metrics=false
enable_gk_flags=false
setup_ms_teams_extensions=false
#skip_configure_redis=True

[all:vars]
# URL of Docker Registry
docker_registry_url="cp1.internal.example.com:5000"
# Address of Kubernetes load balancer 
load_balancer_dns=web1.internal.example.com
# Address of IHS (if one IHS) or DNS name pointing to load balancer pointing to IHSs. 
ic_internal="connections1.internal.example.com"
# dynamicHost
frontend_fqdn="web1.example.com"
frontend_domain="example.com"
db_server="db1.internal.example.com"
db_server_user="EMPINST"
db_server_password="password"
# This version was officially announced for CNX 6.5.0.1. If removed, default it 1.18.10
kubernetes_version=1.17.2
calico_install_latest=True
# This is the version we were supporting CNX 6.5.0.1 on. If removed, Helm v3 (3.4.1) would be installed.
helm_version=2.16.3

# Those variables are needed for installation and postinstallation tasks
dmgr_hostname="connections1.internal.example.com"
was_username=wasadmin
was_password=password
connections_auto_start_clusters='App'

# This needs to match the same variable in your connections inventory file. This will instruct the automation what to map when setting up Prometheus
# and Grafana (controlled with setup_prometheus_operator)
connections_jmx_clusters=[{"name":"App","port":"8080"}]

# This is only for development and playing purposes! This will remove Kubernetes fully from k8s_masters and k8s_workers, including Helm, Docker Registry, NFS folders.
force_destroy_kubernetes=True
