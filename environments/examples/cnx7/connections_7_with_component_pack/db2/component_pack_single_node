# Kubernetes masters. First in the group will become primary Kubernetes master, and used later through the scripts as such.
[k8s_masters]
cp1.internal.example.com

# List of all Kubernetes workers. Those workers will be joined to cluster of masters.
[k8s_workers]
cp1.internal.example.com

# On this host (those hosts) Helm will be set. By default 3.4.1
[k8s_admin]
cp1.internal.example.com

# If you are setting up Docker Registry, first node on the list will get the registry set, certificates created, and all the rest will trust this certificate
# and be able to docker login to the Registry itself.
[docker_registry]
cp1.internal.example.com

# If used, this is where your Haproxy lives.
[k8s_load_balancers]
web1.internal.example.com

# Here will all Component Pack installation steps happen. Can be same as K8S master if masters are available, otherwise any host with kubectl and helm
[component_pack_master]
cp1.internal.example.com

# If you are setting NFS as well for your PV(C)s, first server on the list is going to be NFS master. Add all Kubernetes nodes to the list.
[nfs_servers]
cp1.internal.example.com

[nfs_servers:vars]
nfs_export_netmask=255.255.255.0

# DMGR and WAS nodes are needed for some of installation and post installation steps which include Connections configuration changes.
[dmgr]
connections1.internal.example.com

[was_servers]
connections1.internal.example.com

# This will ensure that Haproxy doesn't clash with Nginx ports when collocated.
[k8s_load_balancers:vars]
main_port='81'
main_ssl_port='444'

# Ensures if you are setting up DR or not. In any case, it will use docker_registry_url as the URL to your Docker Registry.
[docker_registry:vars]
setup_docker_registry=true

# If you are doing PoC/installing Kubernetes and Component Pack on a single node, be sure to set this to true. Remove this altogether for non single node envs.
[k8s_masters:vars]
single_node_installation=true

[component_pack_master:vars]
# If set to true, number of pods in connections namespace will be scaled to the number of total workers in k8s_workers
enable_pod_auto_scaling=True
# Location and name of Component Pack package
component_pack_download_location="c7installer1.internal.example.com:8001/cp"
component_pack_package_name="hybridcloud_latest.zip"
# If you are using different domains, this is needed to set up ingresses properly.
ce_on_prem=False
ingress_multi_domain_enabled=true
# If set to true, Ansible will not check if expected pods are in Running state
skip_pod_checks=true

# Variables bellow are enabling only the steps needed (and existing) in Component Pack for 7.
#setup_installation=false
#setup_images=false
#setup_credentials=false
#setup_connections_volumes=false
#setup_psp=false
#setup_bootstrap=false
#setup_connections_env=false
#setup_infrastructure=false
#setup_customizer=false
#setup_elasticsearch7=false
# Teams is here disabled by default due to issues with compatibility with Helm v3. Talk to HCL support to get the fix. 
setup_teams=false
#setup_tailored_exp=false
# cnx-ingress controller is disabled; instead of that, community one (latest stable one) is being installed.
setup_ingress=false
# This is setting up Nginx ingress from Google's Github, and applying ingress rules from cnx-ingress charts. 
#setup_community_ingress=false
#setup_orientme=false
#setup_dashboards=false
# Elasticstack Helm charts are, in the current CNX 7 version of Component Pack broken. Reach out to HCL support to get working ones if needed. 
setup_elasticstack=false
#setup_sanity=false
#setup_kudosboards=false
# This will set up Prometheus and Grafana. It is not mandatory, but good to have.
#setup_prometheus_operator=false
#setup_outlook_addin=false
#enable_es_metrics=false
#enable_gk_flags=false
# This step is needed if setting up Teams. if setup_teams=false then this also should stay false.
setup_ms_teams_extensions=false
#skip_configure_redis=True

[all:vars]
# URL of Docker Registry
docker_registry_url="cp1.internal.example.com:5000"
# Address of Kubernetes load balancer
load_balancer_dns=web1.internal.example.com
# Address of IHS (if one IHS) or DNS name pointing to load balancer pointing to IHSs.
ic_internal="connections1.internal.example.com"
# dynamicHost
frontend_fqdn="web1.example.com"
frontend_domain="example.com"
db_server="db1.internal.example.com"
db_server_user="db2inst1"
db_server_password="password"
# Latest tested Kubernetes and Docker versions by HCL on the latest stable branch. 
kubernetes_version=1.18.12
docker_version=19.03.13
calico_install_latest=True

# Those variables are needed for installation and postinstallation tasks
dmgr_hostname="connections1.internal.example.com"
was_username=wasadmin
was_password=password
connections_auto_start_clusters='App'

# This needs to match the same variable in your connections inventory file. This will instruct the automation what to map when setting up Prometheus
# and Grafana (controlled with setup_prometheus_operator)
connections_jmx_clusters=[{"name":"App","port":"8080"}]

# This is only for development and playing purposes! This will remove Kubernetes fully from k8s_masters and k8s_workers, including Helm, Docker Registry, NFS folders.
force_destroy_kubernetes=True
