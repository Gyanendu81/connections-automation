# This can be one or multiple Kubernetes masters. The first node of the list becomes primary master
[k8s_masters]
cp1master1.internal.cnx-dev.net

[k8s_workers]
cp1worker1.internal.cnx-dev.net
cp1worker2.internal.cnx-dev.net
cp1worker3.internal.cnx-dev.net

# This is where Helm gets set up. Normaly we do it on the master node. 
[k8s_admin]
cp1master1.internal.cnx-dev.net

# This is the list of all Kubernetes nodes. The first node on the list will get Docker Registry set up and the rest of the
# nodes will have certificate distributed and be configured the way they can log in to the Docker Registry. Docker Registry
# will not be set up if setup_docker_registry is not set up to true, see the section below
[docker_registry]
cp1master1.internal.cnx-dev.net
cp1worker1.internal.cnx-dev.net
cp1worker2.internal.cnx-dev.net
cp1worker3.internal.cnx-dev.net

# This is the node where Haproxy gets set. Normally we colocate it with Nginx
[k8s_load_balancers]
web1.internal.cnx-dev.net

[dmgr]
dmgr1.internal.cnx-dev.net

[was_servers]
connections1.internal.cnx-dev.net
connections2.internal.cnx-dev.net

# If you want, this automation can also set up file server for you using Nginx. This is the node where it would happen.
# It will work only if setup_file_server is set to true.
# Use with caution: Nginx can have issues with serving huge files! 
[installer]
c7installer1.internal.cnx-dev.net

# This is the node where Component Pack gets download and installation handled. Should be the same as the one under k8s_admin
# section.
[component_pack_master]
cp1master1.internal.cnx-dev.net

# If you are setting up NFS server, the first node in the list will become NFS master, get folders needed for PVs created,
# and export proper paths on the same network on which your master NFS lives. Other servers will be set up as clients.
[nfs_servers]
cp1master1.internal.cnx-dev.net
cp1worker1.internal.cnx-dev.net
cp1worker2.internal.cnx-dev.net
cp1worker3.internal.cnx-dev.net

# main_port and main_ssl_port ensure that Haproxy doesn't clashes with Nginx. 
[k8s_load_balancers:vars]
main_port='81'
main_ssl_port='444'

# See the installer section above.
[installer:vars]
setup_file_server=True

# See docker_registry section above.
[docker_registry:vars]
setup_docker_registry=true

# Those are the variables needed for setting up Component Pack.
[component_pack_master:vars]
# If set to true, it will scale the number of pods to the number of servers in k8s_workers section. 
enable_pod_auto_scaling=True
# The location from where Component Pack package can be downloaded
component_pack_download_location="c7installer1.internal.cnx-dev.net:8001/cp"
# The name of Component Pack package on the URL from above
component_pack_package_name="hybridcloud_latest.zip"
# Next two variables are used for connections-env configmap; if you have more then one domain, as most of production
# installations do (for example, your dynamicHost is connections.cnx-dev.net, and all your nodes are on *.internal.cnx-dev.net
# you need to set it this way.
ce_on_prem=False
ingress_multi_domain_enabled=true
# This is for development purposes only
skip_pod_checks=true
# Everything bellow is using its defaults and is set to false. By default ES7 will be installed. You can uncomment one or more
# things if you want to skip them for some reason.
#setup_installation=false
#setup_images=false
#setup_credentials=false
#setup_connections_volumes=false
#setup_psp=false
#setup_bootstrap=false
#setup_connections_env=false
#setup_infrastructure=false
#setup_customizer=false
#setup_elasticsearch=True
#setup_elasticsearch7=false
#setup_teams=false
#setup_tailored_exp=false
#setup_ingress=false
#setup_ingress_rule=false
#setup_community_ingress=false
#setup_orientme=false
#setup_dashboards=false
#setup_elasticstack=false
#setup_sanity=false
#setup_kudosboards=false
#setup_prometheus_operator=false
#setup_outlook_addin=false
#enable_es_metrics=false
#enable_gk_flags=false
#setup_ms_teams_extensions=false
# If uncommented (or set to True) this will instruct bootstrap job not to configure Redis and OrientMe will not work. Use with
# extreme caution!
#skip_configure_redis=True

[all:vars]
# The address of your Docker Registry. Will be passed as value to all Helm charts on installation, and is where from 
# Kubernetes workers are going to try to pull the images.
docker_registry_url="cp1master1.internal.cnx-dev.net:5000"
# The address of your Haproxy
load_balancer_dns=web1.internal.cnx-dev.net
# The address of internal load balancer behind which are IHSs. In case there is no HA here, this is the address of IHS.
ic_internal="ihsvip.cnx-dev.net"
# This is the same what gets set as dynamicHost. This is the front gateway your requests are comming to the system.
frontend_fqdn="web1.cnx-dev.net"
# This is the domain of your dynamicHost. Used for ingress purposes, otherwise ingress will give you 404 in most of production
# situations.
frontend_domain="cnx-dev.net"
# DB information is needed for running the migration on OrientMe postinstall. It will migrate the profiles from DB2 to MongoDB
db_server="db1.internal.cnx-dev.net"
db_server_user="db2inst1"
db_server_password="password"
# This is needed for different installation and post installation tasks for Component Pack. 
dmgr_hostname="dmgr1.internal.cnx-dev.net"
was_username=wasadmin
was_password=password
connections_auto_start_clusters='Apps Push Util Infra'
# This hash is used by Prometheus and Grafana, to map CNX apps and ports. 
connections_jmx_clusters=[{"name":"Apps","port":"8080"}, {"name":"Infra","port":"8081"}, {"name":"Util","port":"8082"}, {"name":"Push","port":"8083"}]
